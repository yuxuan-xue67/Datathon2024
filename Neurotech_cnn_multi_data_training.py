# -*- coding: utf-8 -*-
"""Copy of Neurotech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/192GCTxMO9Igp1FmYrJdj0-2r1EerFQgZ
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import numpy as np
from PIL import Image, ImageOps
import os
import ast
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import tensorflow as tf
from collections import Counter
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout, Reshape, GlobalAveragePooling1D
from sklearn.ensemble import RandomForestClassifier
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

# Add data augmentation to your training data generator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.callbacks import LearningRateScheduler

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

from sklearn.utils import shuffle

from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

eval_a_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Evaluation [NEW]/eval_a_NEW_X.npy')
eval_b_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Evaluation [NEW]/eval_b_NEW_X.npy')

p00_n1_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p00_n1_NEW_X.npy')
p00_n1_y = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p00_n1_NEW_y.npy')

p00_n2_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p00_n2_NEW_X.npy')
p00_n2_y = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p00_n2_NEW_y.npy')

X_train_list = []
y_train_list = []
include_patterns = ['p00', 'p01', 'p02']
X_test = eval_a_X
X_to_match = 'X'
for file in os.listdir(train_path):
  if file.endswith('.npy'):
    file_path = os.path.join(train_path, file)
    if X_to_match in file and any(pattern in file for pattern in include_patterns):
      X_train_list.append(np.load(file_path))
    elif any(pattern in file for pattern in include_patterns):
      y_train_list.append(np.load(file_path))

# Using only one patient's data to train
scaler = StandardScaler()
# Reshape to 2D for scaling
X_train_flat = p00_n1_X.reshape(-1, 6 * 3000)
X_train_scaled = scaler.fit_transform(X_train_flat)
y_train = p00_n1_y

# Ignore some waking epochs during training
waking_indices = np.where(y_train == 1)[0]  # Assuming waking class is labeled as 1
non_waking_indices = np.where(y_train != 1)[0]

# Ensure that the number of non-waking indices is at least as large as the number of waking indices
num_selected_indices = min(len(waking_indices), len(non_waking_indices))

# Randomly sample non-waking indices without replacement
selected_indices = np.random.choice(non_waking_indices, size=num_selected_indices, replace=False)

# Concatenate waking indices to the selected non-waking indices
selected_indices = np.concatenate([selected_indices, waking_indices])

X_train_scaled = X_train_scaled[selected_indices]
y_train = y_train[selected_indices]

# Split the data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_scaled, y_train, test_size=0.2, random_state=42
)

# Neural Network Model
model = Sequential()
model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(3000, 6)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Reshape data for the neural network
X_train_reshaped = X_train_split.reshape(-1, 3000, 6)
X_val_reshaped = X_val_split.reshape(-1, 3000, 6)

# Train the model
model.fit(X_train_reshaped, y_train_split, epochs=5, validation_data=(X_val_reshaped, y_val_split))

# # Evaluate on the test set
# X_test = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p09_n1_NEW_X.npy')
# X_test_flat = X_test.reshape(-1, 6 * 3000)
# X_test_scaled = scaler.transform(X_test_flat)
# X_test_reshaped = X_test_scaled.reshape(-1, 3000, 6)
# nn_predictions = np.argmax(model.predict(X_test_reshaped), axis=-1)

# # Load the true labels for the test set
# y_true = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]/p09_n1_NEW_y.npy')

# # Assuming y_true is already flattened
# accuracy = accuracy_score(y_true, nn_predictions)

# print("Accuracy on the test set:", accuracy)

# Directory containing test files
test_directory = '/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]'

# List to store individual accuracies
individual_accuracies = []

# Loop through each file in the directory
for file_name in os.listdir(test_directory):
    if file_name.endswith('_X.npy'):
        # Form corresponding y file name
        y_file_name = file_name.replace('_X.npy', '_y.npy')

        # Load test data and preprocess
        X_test = np.load(os.path.join(test_directory, file_name))
        X_test_flat = X_test.reshape(-1, 6 * 3000)
        X_test_scaled = scaler.transform(X_test_flat)
        X_test_reshaped = X_test_scaled.reshape(-1, 3000, 6)

        # Make predictions
        nn_predictions = np.argmax(model.predict(X_test_reshaped), axis=-1)

        # Load true labels
        y_true = np.load(os.path.join(test_directory, y_file_name))

        # Compute accuracy for the current file
        accuracy = accuracy_score(y_true, nn_predictions)
        individual_accuracies.append(accuracy)

        # Print or store any additional information or metrics if needed
        print(f"Accuracy for {file_name}: {accuracy}")

# Calculate mean accuracy
mean_accuracy = np.mean(individual_accuracies)
print(f"Mean Accuracy across all files: {mean_accuracy}")

# Improve the training dataset by Downsample
from collections import Counter

y_train = p00_n1_y

# Find the count of the next most frequent class
class_distribution = Counter(y_train)
most_common_classes = class_distribution.most_common()
next_most_common_count = most_common_classes[1][1]

# Number of samples to keep from class '1'
n_samples_to_keep = next_most_common_count
# Indices of class '1' and other classes
indices_class_1 = np.where(y_train == 1)[0]
indices_other_classes = np.where(y_train != 1)[0]

# Randomly select samples from class '1'
np.random.shuffle(indices_class_1)
indices_class_1_downsampled = indices_class_1[:n_samples_to_keep]

# Combine and shuffle indices
downsampled_indices = np.concatenate([indices_class_1_downsampled, indices_other_classes])
np.random.shuffle(downsampled_indices)

# Create the downsampled dataset
X_train = p00_n1_X
X_downsampled = X_train[downsampled_indices]
y_downsampled = y_train[downsampled_indices]

# Checking the new distribution
new_class_distribution = Counter(y_downsampled)

# Modeling
scaler = StandardScaler()
# Reshape to 2D for scaling
X_train_flat = X_downsampled.reshape(-1, 6 * 3000)
X_train_scaled = scaler.fit_transform(X_train_flat)

# Split the data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_scaled, y_downsampled, test_size=0.2, random_state=42
)

# Neural Network Model
model = Sequential()
model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(3000, 6)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Reshape data for the neural network
X_train_reshaped = X_train_split.reshape(-1, 3000, 6)
X_val_reshaped = X_val_split.reshape(-1, 3000, 6)

# Train the model
model.fit(X_train_reshaped, y_train_split, epochs=5, validation_data=(X_val_reshaped, y_val_split))

# Directory containing test files
test_directory = '/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]'

# List to store individual accuracies
individual_accuracies = []

# Loop through each file in the directory
for file_name in os.listdir(test_directory):
    if file_name.endswith('_X.npy'):
        # Form corresponding y file name
        y_file_name = file_name.replace('_X.npy', '_y.npy')

        # Load test data and preprocess
        X_test = np.load(os.path.join(test_directory, file_name))
        X_test_flat = X_test.reshape(-1, 6 * 3000)
        X_test_scaled = scaler.transform(X_test_flat)
        X_test_reshaped = X_test_scaled.reshape(-1, 3000, 6)

        # Make predictions
        nn_predictions = np.argmax(model.predict(X_test_reshaped), axis=-1)

        # Load true labels
        y_true = np.load(os.path.join(test_directory, y_file_name))

        # Compute accuracy for the current file
        accuracy = accuracy_score(y_true, nn_predictions)
        individual_accuracies.append(accuracy)

        # Print or store any additional information or metrics if needed
        print(f"Accuracy for {file_name}: {accuracy}")

# Calculate mean accuracy
mean_accuracy = np.mean(individual_accuracies)
print(f"Mean Accuracy across all files: {mean_accuracy}")

# Utilize 4 patient's data to train the model.
# Directory containing training files
train_directory = '/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]'

# List of six specific persons
selected_persons = ['p01', 'p02', 'p03', 'p04']

# Initialize lists to store data and labels for selected persons
X_selected = []
y_selected = []

for file_name in os.listdir(train_directory):
    if any(person in file_name for person in selected_persons):
      if file_name.endswith('_X.npy'):
          # Form corresponding y file name
          y_file_name = file_name.replace('_X.npy', '_y.npy')
          y_selected.append(np.load(os.path.join(train_directory, y_file_name)))
          X_selected.append(np.load(os.path.join(train_directory, file_name)))


# Concatenate data and labels for selected persons
X_combined = np.concatenate(X_selected, axis=0)
y_combined = np.concatenate(y_selected, axis=0)

# Find the count of the next most frequent class
class_distribution = Counter(y_combined)
most_common_classes = class_distribution.most_common()
next_most_common_count = most_common_classes[1][1]

# Number of samples to keep from class '1'
n_samples_to_keep = next_most_common_count
# Indices of class '1' and other classes
indices_class_1 = np.where(y_combined == 1)[0]
indices_other_classes = np.where(y_combined != 1)[0]

# Randomly select samples from class '1'
np.random.shuffle(indices_class_1)
indices_class_1_downsampled = indices_class_1[:n_samples_to_keep]

# Combine and shuffle indices
downsampled_indices = np.concatenate([indices_class_1_downsampled, indices_other_classes])
np.random.shuffle(downsampled_indices)

# Create the downsampled dataset
X_downsampled = X_combined[downsampled_indices]

X_resized = X_downsampled[:, :3, :]

y_downsampled = y_combined[downsampled_indices]

# Checking the new distribution
new_class_distribution = Counter(y_downsampled)

# Modeling
scaler = StandardScaler()

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/drive/My Drive/Datathon 2024/channel6.csv')

# Extract data under the "Processed Channel Data" column
X_train = df['Processed Channel Data']
y_train = df['Labels']

# X_train_flat = X_train.reshape(-1, 3 * 3000)
# X_train_scaled = scaler.fit_transform(X_train_flat)

# Split the data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

print("Size of X_train_split:", len(X_train_split))
print("Size of X_val_split:", len(X_val_split))
print("Size of y_train_split:", len(y_train_split))
print("Size of y_val_split:", len(y_val_split))


# Neural Network Model
model = Sequential()
model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(100, 1)))
model.add(Conv1D(64, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(7, activation='softmax'))
# model.add(Dropout(0.5))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Assuming each element is a 1D array of size 100
# X_train_reshaped = np.vstack(X_train_split.values).reshape(100)
# X_val_reshaped = np.vstack(X_val_split.values).reshape(100)

# # Check the shapes
# print("Shape of X_train_reshaped:", X_train_reshaped.shape)
# print("Shape of X_val_reshaped:", X_val_reshaped.shape)

# Using learning rate scheduler to improve the efficiency
def lr_schedule(epoch):
    lr = 0.001
    if epoch > 3:
        lr *= 0.1
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)

model.fit(X_train_split, y_train_split, epochs=5, batch_size=32, validation_data=(X_val_split, y_val_split), callbacks=[lr_scheduler])

# Split channel 6 to train the model
# Modeling
scaler = StandardScaler()

# Read the CSV file into a DataFrame
df = pd.read_csv('/content/drive/My Drive/Datathon 2024/channel6.csv')

# Extract data under the "Processed Channel Data" column
X_train = df['Processed Channel Data']
y_train = df['Labels']

# Assuming X_train, y_train are already defined
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Assuming X_train_split and X_val_split are Series containing string representations of arrays
X_train_numeric = X_train.apply(lambda x: np.array(ast.literal_eval(x)))
X_val_numeric = X_val.apply(lambda x: np.array(ast.literal_eval(x)))

# Flatten the arrays if needed
X_train_flatten = np.vstack(X_train_numeric.apply(lambda x: x.flatten()))
X_val_flatten = np.vstack(X_val_numeric.apply(lambda x: x.flatten()))

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train_flatten, y_train)

# Make predictions on the validation set
y_val_pred = rf_model.predict(X_val_flatten)

# Evaluate the accuracy
accuracy = accuracy_score(y_val, y_val_pred)
print("Validation Accuracy:", accuracy)

# Train the model with 6 patient's data
# Directory containing test files
test_directory = '/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Training [NEW]'

# List of six specific persons
selected_persons_test = ['p09', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p06', 'p07', 'p08', 'p19', 'p20']

# List to store individual accuracies
individual_accuracies = []

for file_name in os.listdir(test_directory):
    if any(person in file_name for person in selected_persons_test):
      if file_name.endswith('_X.npy'):
        # Form corresponding y file name
        y_file_name = file_name.replace('_X.npy', '_y.npy')

        # Load test data and preprocess
        X_test = np.load(os.path.join(test_directory, file_name))
        X_test_flat = X_test.reshape(-1, 6 * 3000)
        X_test_scaled = scaler.transform(X_test_flat)
        X_test_reshaped = X_test_scaled.reshape(-1, 3000, 6)

        # Make predictions
        nn_predictions = np.argmax(model.predict(X_test_reshaped), axis=-1)

        # Load true labels
        y_true = np.load(os.path.join(test_directory, y_file_name))

        # Compute accuracy for the current file
        accuracy = accuracy_score(y_true, nn_predictions)
        individual_accuracies.append(accuracy)

        # Print or store any additional information or metrics if needed
        print(f"Accuracy for {file_name}: {accuracy}")

# Calculate mean accuracy
mean_accuracy = np.mean(individual_accuracies)
print(f"Mean Accuracy across all files: {mean_accuracy}")

eval_a_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Evaluation [NEW]/eval_a_NEW_X.npy')
eval_b_X = np.load('/content/drive/My Drive/Datathon 2024/Neurotech@Rice Datathon Challenge/Evaluation [NEW]/eval_b_NEW_X.npy')

# Load test data
X_test = eval_a_X
eval_a_X_resized = eval_a_X[:, :3, :]
X_test_flat = eval_a_X_resized.reshape(-1, 6 * 3000)
X_test_scaled = scaler.transform(X_test_flat)
X_test_reshaped = X_test_scaled.reshape(-1, 3000, 6)
nn_predictions = np.argmax(model.predict(X_test_reshaped), axis=-1)
print(nn_predictions)

plt.plot(nn_predictions)
plt.title('Predicted Sleep Stages')
plt.xlabel('Epoch')
plt.ylabel('Predicted Stage')
plt.show()